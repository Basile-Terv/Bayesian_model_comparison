{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "paths data_path\n",
      "/Users/Lea/Documents/Documents_2/Documents/MVA/S2/BML/Projet/Bayesian_model_comparison/DKL_experiments/data_2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Lea/anaconda3/envs/opt_transport/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from data import regression_datasets, get_regression_data\n",
    "import DKLUtils \n",
    "import gpytorch\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from torch.optim import SGD, Adam\n",
    "from torch.optim.lr_scheduler import MultiStepLR\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tqdm\n",
    "import seaborn as sns\n",
    "import argparse\n",
    "\n",
    "from gpytorch.models import ApproximateGP\n",
    "from gpytorch.variational import CholeskyVariationalDistribution\n",
    "from gpytorch.variational import VariationalStrategy\n",
    "from botorch.models import SingleTaskVariationalGP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get_regression_data('naval')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mll(gp, x, y):\n",
    "    N = len(x)\n",
    "    x = gp.feature_extractor(x)\n",
    "    x = gp.scale_to_bounds(x)\n",
    "    covar_matrix = gp.covar_module(x,x).evaluate()\n",
    "    covar_matrix += gp.likelihood.noise * torch.eye(N).to(x.device)\n",
    "    log_mll = - 0.5 * (y.T @ torch.inverse(covar_matrix)) @ y \n",
    "    log_mll += - 0.5 * torch.logdet(covar_matrix)\n",
    "    log_mll += - 0.5 * N * np.log(2 * np.pi)\n",
    "\n",
    "    return log_mll\n",
    "\n",
    "def CondtionalMLL(gp, x, y, xm, ym):\n",
    "    return get_mll(gp, x, y) - get_mll(gp, xm, ym)\n",
    "\n",
    "def RMSE(preds, targets):\n",
    "    return (preds.squeeze().cpu() - targets.squeeze().cpu()).pow(2).mean().pow(0.5)\n",
    "\n",
    "\n",
    "class LargeFeatureExtractor(torch.nn.Sequential):\n",
    "    def __init__(self, data_dim):\n",
    "        super(LargeFeatureExtractor, self).__init__()\n",
    "        self.add_module('linear3', torch.nn.Linear(data_dim, 50))\n",
    "        self.add_module('relu3', torch.nn.ReLU())\n",
    "        self.add_module('linear4', torch.nn.Linear(50, 2))\n",
    "\n",
    "class GPRegressionModel(gpytorch.models.ExactGP):\n",
    "        def __init__(self, train_x, train_y, likelihood, feature_extractor):\n",
    "            super(GPRegressionModel, self).__init__(train_x, train_y, likelihood)\n",
    "            self.mean_module = gpytorch.means.ConstantMean()\n",
    "            self.covar_module = gpytorch.kernels.GridInterpolationKernel(\n",
    "                gpytorch.kernels.ScaleKernel(gpytorch.kernels.RBFKernel(ard_num_dims=2)),\n",
    "                num_dims=2, grid_size=100\n",
    "            )\n",
    "            self.feature_extractor = feature_extractor\n",
    "\n",
    "            # This module will scale the NN features so that they're nice values\n",
    "            self.scale_to_bounds = gpytorch.utils.grid.ScaleToBounds(-1., 1.)\n",
    "\n",
    "        def forward(self, x):\n",
    "            # We're first putting our data through a deep net (feature extractor)\n",
    "            projected_x = self.feature_extractor(x)\n",
    "            projected_x = self.scale_to_bounds(projected_x)  # Make the NN values \"nice\"\n",
    "\n",
    "            mean_x = self.mean_module(projected_x)\n",
    "            covar_x = self.covar_module(projected_x)\n",
    "            return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)\n",
    "        \n",
    "        \n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(m_prop, losstype,dataset,ntrain=200,ntrial=10):\n",
    "    \n",
    "    data = get_regression_data(dataset)\n",
    "    train_x = torch.FloatTensor(data.X_train)[:ntrain]\n",
    "    train_y = torch.FloatTensor(data.Y_train).squeeze()[:ntrain]\n",
    "    test_x = torch.FloatTensor(data.X_test)\n",
    "    test_y = torch.FloatTensor(data.Y_test).squeeze()\n",
    "    data_dim = train_x.size(-1)\n",
    "    m = int(m_prop * train_x.shape[0])\n",
    "    \n",
    "    \n",
    "    rmse = torch.zeros(ntrial)\n",
    "    for trl in range(ntrial):\n",
    "\n",
    "        feature_extractor = LargeFeatureExtractor(data_dim)\n",
    "        likelihood = gpytorch.likelihoods.GaussianLikelihood()\n",
    "        model = GPRegressionModel(train_x, train_y, likelihood,\n",
    "                                  feature_extractor)\n",
    "\n",
    "        if torch.cuda.is_available():\n",
    "            use_cuda=True\n",
    "            model = model.cuda()\n",
    "            likelihood = likelihood.cuda()\n",
    "            train_x, train_y, test_x, test_y = train_x.cuda(), train_y.cuda(), test_x.cuda(), test_y.cuda()\n",
    "\n",
    "        training_iterations = 100\n",
    "\n",
    "        # Find optimal model hyperparameters\n",
    "        model.train()\n",
    "        likelihood.train()\n",
    "\n",
    "        # Use the adam optimizer\n",
    "        optimizer = torch.optim.Adam([\n",
    "            {'params': model.feature_extractor.parameters()},\n",
    "            {'params': model.covar_module.parameters()},\n",
    "            {'params': model.mean_module.parameters()},\n",
    "            {'params': model.likelihood.parameters()},\n",
    "        ], lr=0.01)\n",
    "\n",
    "        # \"Loss\" for GPs - the marginal log likelihood\n",
    "        mll = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood, model)\n",
    "\n",
    "        def train(losstype='cmll'):\n",
    "            iterator = tqdm.tqdm(range(training_iterations))\n",
    "            for i in iterator:\n",
    "                # Zero backprop gradients\n",
    "                optimizer.zero_grad()\n",
    "                # Get output from model\n",
    "                output = model(train_x)\n",
    "                # Calc loss and backprop derivatives\n",
    "                if losstype=='mll':\n",
    "                    loss = -mll(output, train_y)\n",
    "                if losstype=='cmll':\n",
    "                    order = torch.randperm(train_x.shape[0])        \n",
    "                    xm = train_x[order[:m]]\n",
    "                    xstar = train_x[order[m:]]\n",
    "\n",
    "                    ym = train_y[order[:m]]\n",
    "                    ystar = train_y[order[m:]]\n",
    "                    loss = -CondtionalMLL(model, train_x, train_y, xm, ym)\n",
    "\n",
    "                loss.backward()\n",
    "                iterator.set_postfix(loss=loss.item())\n",
    "                optimizer.step()\n",
    "\n",
    "        train(losstype=losstype)\n",
    "        model.eval();\n",
    "        test_preds = model(test_x).mean\n",
    "        rmse[trl] = RMSE(test_preds, test_y)\n",
    "        \n",
    "    fpath = \"./saved-outputs/\"\n",
    "    fname = \"exactdkl\" + dataset + \"_ntrain\" + str(ntrain) + \"_\" + losstype\n",
    "    if losstype == \"cmll\":\n",
    "        fname += \"_\" + str(m_prop) + \"m\"\n",
    "    fname += \".pt\"\n",
    "    print(fpath+fname)\n",
    "    \n",
    "    torch.save(rmse, fpath + fname)\n",
    "    #print('model saved')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/100 [00:00<?, ?it/s]/Users/Lea/anaconda3/envs/opt_transport/lib/python3.12/site-packages/linear_operator/utils/interpolation.py:71: UserWarning: torch.sparse.SparseTensor(indices, values, shape, *, device=) is deprecated.  Please use torch.sparse_coo_tensor(indices, values, shape, dtype=, device=). (Triggered internally at /Users/runner/work/_temp/anaconda/conda-bld/pytorch_1708025535429/work/torch/csrc/utils/tensor_new.cpp:620.)\n",
      "  summing_matrix = cls(summing_matrix_indices, summing_matrix_values, size)\n",
      "/var/folders/4l/cx3vxzys6jgblv_bdlfs946c0000gn/T/ipykernel_74707/2711892513.py:7: UserWarning: The use of `x.T` on tensors of dimension other than 2 to reverse their shape is deprecated and it will throw an error in a future release. Consider `x.mT` to transpose batches of matrices or `x.permute(*torch.arange(x.ndim - 1, -1, -1))` to reverse the dimensions of a tensor. (Triggered internally at /Users/runner/work/_temp/anaconda/conda-bld/pytorch_1708025535429/work/aten/src/ATen/native/TensorShape.cpp:3641.)\n",
      "  log_mll = - 0.5 * (y.T @ torch.inverse(covar_matrix)) @ y\n",
      " 21%|██        | 21/100 [00:05<00:18,  4.25it/s, loss=116][E thread_pool.cpp:130] Exception in thread pool task: mutex lock failed: Invalid argument\n",
      "[E thread_pool.cpp:130] Exception in thread pool task: mutex lock failed: Invalid argument\n",
      "[E thread_pool.cpp:130] Exception in thread pool task: mutex lock failed: Invalid argument\n",
      "[E thread_pool.cpp:130] Exception in thread pool task: mutex lock failed: Invalid argument\n",
      "[E thread_pool.cpp:130] Exception in thread pool task: mutex lock failed: Invalid argument\n",
      " 21%|██        | 21/100 [00:05<00:19,  3.99it/s, loss=116]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m m \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0.1\u001b[39m,\u001b[38;5;241m0.25\u001b[39m,\u001b[38;5;241m0.5\u001b[39m,\u001b[38;5;241m0.75\u001b[39m,\u001b[38;5;241m0.9\u001b[39m}:\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m ntrain \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m100\u001b[39m,\u001b[38;5;241m200\u001b[39m,\u001b[38;5;241m300\u001b[39m,\u001b[38;5;241m400\u001b[39m,\u001b[38;5;241m500\u001b[39m,\u001b[38;5;241m600\u001b[39m,\u001b[38;5;241m700\u001b[39m}:\n\u001b[0;32m----> 5\u001b[0m         \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcmll\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mdset\u001b[49m\u001b[43m,\u001b[49m\u001b[43mntrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43mntrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m ntrain \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m100\u001b[39m,\u001b[38;5;241m200\u001b[39m,\u001b[38;5;241m300\u001b[39m,\u001b[38;5;241m400\u001b[39m,\u001b[38;5;241m500\u001b[39m,\u001b[38;5;241m600\u001b[39m,\u001b[38;5;241m700\u001b[39m}:\n\u001b[1;32m      8\u001b[0m     main(m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmll\u001b[39m\u001b[38;5;124m'\u001b[39m,dset,ntrain,ntrial\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m)\n",
      "Cell \u001b[0;32mIn[6], line 66\u001b[0m, in \u001b[0;36mmain\u001b[0;34m(m_prop, losstype, dataset, ntrain, ntrial)\u001b[0m\n\u001b[1;32m     63\u001b[0m         iterator\u001b[38;5;241m.\u001b[39mset_postfix(loss\u001b[38;5;241m=\u001b[39mloss\u001b[38;5;241m.\u001b[39mitem())\n\u001b[1;32m     64\u001b[0m         optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m---> 66\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlosstype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlosstype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     67\u001b[0m model\u001b[38;5;241m.\u001b[39meval();\n\u001b[1;32m     68\u001b[0m test_preds \u001b[38;5;241m=\u001b[39m model(test_x)\u001b[38;5;241m.\u001b[39mmean\n",
      "Cell \u001b[0;32mIn[6], line 62\u001b[0m, in \u001b[0;36mmain.<locals>.train\u001b[0;34m(losstype)\u001b[0m\n\u001b[1;32m     59\u001b[0m     ystar \u001b[38;5;241m=\u001b[39m train_y[order[m:]]\n\u001b[1;32m     60\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39mCondtionalMLL(model, train_x, train_y, xm, ym)\n\u001b[0;32m---> 62\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     63\u001b[0m iterator\u001b[38;5;241m.\u001b[39mset_postfix(loss\u001b[38;5;241m=\u001b[39mloss\u001b[38;5;241m.\u001b[39mitem())\n\u001b[1;32m     64\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[0;32m~/anaconda3/envs/opt_transport/lib/python3.12/site-packages/torch/_tensor.py:522\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    512\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    513\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    514\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    515\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    520\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    521\u001b[0m     )\n\u001b[0;32m--> 522\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    523\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    524\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/opt_transport/lib/python3.12/site-packages/torch/autograd/__init__.py:266\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    261\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    263\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    264\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 266\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    267\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/opt_transport/lib/python3.12/site-packages/torch/autograd/function.py:289\u001b[0m, in \u001b[0;36mBackwardCFunction.apply\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    283\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    284\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mImplementing both \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbackward\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m and \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvjp\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m for a custom \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    285\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFunction is not allowed. You should only implement one \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    286\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mof them.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    287\u001b[0m     )\n\u001b[1;32m    288\u001b[0m user_fn \u001b[38;5;241m=\u001b[39m vjp_fn \u001b[38;5;28;01mif\u001b[39;00m vjp_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m Function\u001b[38;5;241m.\u001b[39mvjp \u001b[38;5;28;01melse\u001b[39;00m backward_fn\n\u001b[0;32m--> 289\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43muser_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/opt_transport/lib/python3.12/site-packages/linear_operator/functions/_matmul.py:46\u001b[0m, in \u001b[0;36mMatmul.backward\u001b[0;34m(ctx, grad_output)\u001b[0m\n\u001b[1;32m     44\u001b[0m     rhs \u001b[38;5;241m=\u001b[39m rhs\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m (rhs\u001b[38;5;241m.\u001b[39mndimension() \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m rhs\n\u001b[1;32m     45\u001b[0m     grad_output_matrix \u001b[38;5;241m=\u001b[39m grad_output\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m grad_output\u001b[38;5;241m.\u001b[39mndimension() \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m grad_output\n\u001b[0;32m---> 46\u001b[0m     arg_grads \u001b[38;5;241m=\u001b[39m \u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrepresentation_tree\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmatrix_args\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_bilinear_derivative\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgrad_output_matrix\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrhs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;66;03m# input_2 gradient\u001b[39;00m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ctx\u001b[38;5;241m.\u001b[39mneeds_input_grad[\u001b[38;5;241m1\u001b[39m]:\n",
      "File \u001b[0;32m~/anaconda3/envs/opt_transport/lib/python3.12/site-packages/linear_operator/operators/_linear_operator.py:380\u001b[0m, in \u001b[0;36mLinearOperator._bilinear_derivative\u001b[0;34m(self, left_vecs, right_vecs)\u001b[0m\n\u001b[1;32m    378\u001b[0m     loss \u001b[38;5;241m=\u001b[39m (left_vecs \u001b[38;5;241m*\u001b[39m lin_op\u001b[38;5;241m.\u001b[39m_matmul(right_vecs))\u001b[38;5;241m.\u001b[39msum()\n\u001b[1;32m    379\u001b[0m     args_with_grads \u001b[38;5;241m=\u001b[39m [arg \u001b[38;5;28;01mfor\u001b[39;00m arg \u001b[38;5;129;01min\u001b[39;00m args \u001b[38;5;28;01mif\u001b[39;00m arg\u001b[38;5;241m.\u001b[39mrequires_grad]\n\u001b[0;32m--> 380\u001b[0m     actual_grads \u001b[38;5;241m=\u001b[39m deque(\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgrad\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs_with_grads\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mallow_unused\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m)\n\u001b[1;32m    382\u001b[0m \u001b[38;5;66;03m# Now make sure that the object we return has one entry for every item in args\u001b[39;00m\n\u001b[1;32m    383\u001b[0m grads \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[0;32m~/anaconda3/envs/opt_transport/lib/python3.12/site-packages/torch/autograd/__init__.py:411\u001b[0m, in \u001b[0;36mgrad\u001b[0;34m(outputs, inputs, grad_outputs, retain_graph, create_graph, only_inputs, allow_unused, is_grads_batched, materialize_grads)\u001b[0m\n\u001b[1;32m    407\u001b[0m     result \u001b[38;5;241m=\u001b[39m _vmap_internals\u001b[38;5;241m.\u001b[39m_vmap(vjp, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m, allow_none_pass_through\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)(\n\u001b[1;32m    408\u001b[0m         grad_outputs_\n\u001b[1;32m    409\u001b[0m     )\n\u001b[1;32m    410\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 411\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    412\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    413\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrad_outputs_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    414\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    415\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    416\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    417\u001b[0m \u001b[43m        \u001b[49m\u001b[43mallow_unused\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    418\u001b[0m \u001b[43m        \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    419\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    420\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m materialize_grads:\n\u001b[1;32m    421\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28many\u001b[39m(\n\u001b[1;32m    422\u001b[0m         result[i] \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_tensor_like(inputs[i])\n\u001b[1;32m    423\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(inputs))\n\u001b[1;32m    424\u001b[0m     ):\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for dset in {'protein','naval','concrete','winered','winewhite'}:\n",
    "    get_regression_data(dset)\n",
    "    for m in {0.1,0.25,0.5,0.75,0.9}:\n",
    "        for ntrain in {100,200,300,400,500,600,700}:\n",
    "            main(m, 'cmll',dset,ntrain,ntrial=10)\n",
    "            \n",
    "    for ntrain in {100,200,300,400,500,600,700}:\n",
    "        main(m, 'mll',dset,ntrain,ntrial=10)\n",
    "print('The training of the models is finished')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "opt_transport",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
