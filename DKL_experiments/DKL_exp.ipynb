{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "paths data_path\n",
      "/Users/Lea/Documents/Documents_2/Documents/MVA/S2/BML/Projet/Bayesian_model_comparison/DKL_experiments/data_2\n"
     ]
    }
   ],
   "source": [
    "from data import regression_datasets, get_regression_data\n",
    "import DKLUtils \n",
    "import gpytorch\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from torch.optim import SGD, Adam\n",
    "from torch.optim.lr_scheduler import MultiStepLR\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tqdm\n",
    "import seaborn as sns\n",
    "import argparse\n",
    "\n",
    "from gpytorch.models import ApproximateGP\n",
    "from gpytorch.variational import CholeskyVariationalDistribution\n",
    "from gpytorch.variational import VariationalStrategy\n",
    "from botorch.models import SingleTaskVariationalGP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:donwloading naval data\n",
      "INFO:root:finished donwloading naval data\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<data.Naval at 0x2848f2660>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#get_regression_data('naval')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mll(gp, x, y):\n",
    "    N = len(x)\n",
    "    x = gp.feature_extractor(x)\n",
    "    x = gp.scale_to_bounds(x)\n",
    "    covar_matrix = gp.covar_module(x,x).evaluate()\n",
    "    covar_matrix += gp.likelihood.noise * torch.eye(N).to(x.device)\n",
    "    log_mll = - 0.5 * (y.T @ torch.inverse(covar_matrix)) @ y \n",
    "    log_mll += - 0.5 * torch.logdet(covar_matrix)\n",
    "    log_mll += - 0.5 * N * np.log(2 * np.pi)\n",
    "\n",
    "    return log_mll\n",
    "\n",
    "def CondtionalMLL(gp, x, y, xm, ym):\n",
    "    return get_mll(gp, x, y) - get_mll(gp, xm, ym)\n",
    "\n",
    "def RMSE(preds, targets):\n",
    "    return (preds.squeeze().cpu() - targets.squeeze().cpu()).pow(2).mean().pow(0.5)\n",
    "\n",
    "\n",
    "class LargeFeatureExtractor(torch.nn.Sequential):\n",
    "    def __init__(self, data_dim):\n",
    "        super(LargeFeatureExtractor, self).__init__()\n",
    "        self.add_module('linear3', torch.nn.Linear(data_dim, 50))\n",
    "        self.add_module('relu3', torch.nn.ReLU())\n",
    "        self.add_module('linear4', torch.nn.Linear(50, 2))\n",
    "\n",
    "class GPRegressionModel(gpytorch.models.ExactGP):\n",
    "        def __init__(self, train_x, train_y, likelihood, feature_extractor):\n",
    "            super(GPRegressionModel, self).__init__(train_x, train_y, likelihood)\n",
    "            self.mean_module = gpytorch.means.ConstantMean()\n",
    "            self.covar_module = gpytorch.kernels.GridInterpolationKernel(\n",
    "                gpytorch.kernels.ScaleKernel(gpytorch.kernels.RBFKernel(ard_num_dims=2)),\n",
    "                num_dims=2, grid_size=100\n",
    "            )\n",
    "            self.feature_extractor = feature_extractor\n",
    "\n",
    "            # This module will scale the NN features so that they're nice values\n",
    "            self.scale_to_bounds = gpytorch.utils.grid.ScaleToBounds(-1., 1.)\n",
    "\n",
    "        def forward(self, x):\n",
    "            # We're first putting our data through a deep net (feature extractor)\n",
    "            projected_x = self.feature_extractor(x)\n",
    "            projected_x = self.scale_to_bounds(projected_x)  # Make the NN values \"nice\"\n",
    "\n",
    "            mean_x = self.mean_module(projected_x)\n",
    "            covar_x = self.covar_module(projected_x)\n",
    "            return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)\n",
    "        \n",
    "        \n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(m_prop, losstype,dataset,ntrain=200,ntrial=10):\n",
    "    \n",
    "    data = get_regression_data(dataset)\n",
    "    train_x = torch.FloatTensor(data.X_train)[:ntrain]\n",
    "    train_y = torch.FloatTensor(data.Y_train).squeeze()[:ntrain]\n",
    "    test_x = torch.FloatTensor(data.X_test)\n",
    "    test_y = torch.FloatTensor(data.Y_test).squeeze()\n",
    "    data_dim = train_x.size(-1)\n",
    "    m = int(m_prop * train_x.shape[0])\n",
    "    \n",
    "    \n",
    "    rmse = torch.zeros(ntrial)\n",
    "    for trl in range(ntrial):\n",
    "\n",
    "        feature_extractor = LargeFeatureExtractor(data_dim)\n",
    "        likelihood = gpytorch.likelihoods.GaussianLikelihood()\n",
    "        model = GPRegressionModel(train_x, train_y, likelihood,\n",
    "                                  feature_extractor)\n",
    "\n",
    "        if torch.cuda.is_available():\n",
    "            use_cuda=True\n",
    "            model = model.cuda()\n",
    "            likelihood = likelihood.cuda()\n",
    "            train_x, train_y, test_x, test_y = train_x.cuda(), train_y.cuda(), test_x.cuda(), test_y.cuda()\n",
    "\n",
    "        training_iterations = 100\n",
    "\n",
    "        # Find optimal model hyperparameters\n",
    "        model.train()\n",
    "        likelihood.train()\n",
    "\n",
    "        # Use the adam optimizer\n",
    "        optimizer = torch.optim.Adam([\n",
    "            {'params': model.feature_extractor.parameters()},\n",
    "            {'params': model.covar_module.parameters()},\n",
    "            {'params': model.mean_module.parameters()},\n",
    "            {'params': model.likelihood.parameters()},\n",
    "        ], lr=0.01)\n",
    "\n",
    "        # \"Loss\" for GPs - the marginal log likelihood\n",
    "        mll = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood, model)\n",
    "\n",
    "        def train(losstype='cmll'):\n",
    "            iterator = tqdm.tqdm(range(training_iterations))\n",
    "            for i in iterator:\n",
    "                # Zero backprop gradients\n",
    "                optimizer.zero_grad()\n",
    "                # Get output from model\n",
    "                output = model(train_x)\n",
    "                # Calc loss and backprop derivatives\n",
    "                if losstype=='mll':\n",
    "                    loss = -mll(output, train_y)\n",
    "                if losstype=='cmll':\n",
    "                    order = torch.randperm(train_x.shape[0])        \n",
    "                    xm = train_x[order[:m]]\n",
    "                    xstar = train_x[order[m:]]\n",
    "\n",
    "                    ym = train_y[order[:m]]\n",
    "                    ystar = train_y[order[m:]]\n",
    "                    loss = -CondtionalMLL(model, train_x, train_y, xm, ym)\n",
    "\n",
    "                loss.backward()\n",
    "                iterator.set_postfix(loss=loss.item())\n",
    "                optimizer.step()\n",
    "\n",
    "        train(losstype=losstype)\n",
    "        model.eval();\n",
    "        test_preds = model(test_x).mean\n",
    "        rmse[trl] = RMSE(test_preds, test_y)\n",
    "        \n",
    "    fpath = \"./saved-outputs/\"\n",
    "    fname = \"exactdkl\" + dataset + \"_ntrain\" + str(ntrain) + \"_\" + losstype\n",
    "    if losstype == \"cmll\":\n",
    "        fname += \"_\" + str(m_prop) + \"m\"\n",
    "    fname += \".pt\"\n",
    "    print(fpath+fname)\n",
    "    \n",
    "    torch.save(rmse, fpath + fname)\n",
    "    print('model saved')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:23<00:00,  4.21it/s, loss=35.7]\n",
      "100%|██████████| 100/100 [00:24<00:00,  4.15it/s, loss=36.7]\n",
      "100%|██████████| 100/100 [00:23<00:00,  4.22it/s, loss=36.2]\n",
      "100%|██████████| 100/100 [00:23<00:00,  4.20it/s, loss=36.1]\n",
      "100%|██████████| 100/100 [00:23<00:00,  4.19it/s, loss=40.4]\n",
      "100%|██████████| 100/100 [00:24<00:00,  4.16it/s, loss=37.3]\n",
      "100%|██████████| 100/100 [00:24<00:00,  4.14it/s, loss=35.3]\n",
      "100%|██████████| 100/100 [00:23<00:00,  4.25it/s, loss=37.4]\n",
      "100%|██████████| 100/100 [00:24<00:00,  4.11it/s, loss=35.2]\n",
      "100%|██████████| 100/100 [00:24<00:00,  4.14it/s, loss=35.9]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./saved-outputs-bis/exactdklconcrete_ntrain100_cmll_0.1m.pt\n",
      "model saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:46<00:00,  2.13it/s, loss=66.3]\n",
      "100%|██████████| 100/100 [00:47<00:00,  2.13it/s, loss=67.3]\n",
      " 29%|██▉       | 29/100 [00:13<00:33,  2.09it/s, loss=143]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m m \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0.1\u001b[39m,\u001b[38;5;241m0.25\u001b[39m,\u001b[38;5;241m0.5\u001b[39m,\u001b[38;5;241m0.75\u001b[39m,\u001b[38;5;241m0.9\u001b[39m}:\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m ntrain \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m100\u001b[39m,\u001b[38;5;241m200\u001b[39m,\u001b[38;5;241m300\u001b[39m,\u001b[38;5;241m400\u001b[39m,\u001b[38;5;241m500\u001b[39m,\u001b[38;5;241m600\u001b[39m,\u001b[38;5;241m700\u001b[39m}:\n\u001b[0;32m----> 5\u001b[0m         \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcmll\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mdset\u001b[49m\u001b[43m,\u001b[49m\u001b[43mntrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43mntrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m ntrain \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m100\u001b[39m,\u001b[38;5;241m200\u001b[39m,\u001b[38;5;241m300\u001b[39m,\u001b[38;5;241m400\u001b[39m,\u001b[38;5;241m500\u001b[39m,\u001b[38;5;241m600\u001b[39m,\u001b[38;5;241m700\u001b[39m}:\n\u001b[1;32m      8\u001b[0m     main(m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmll\u001b[39m\u001b[38;5;124m'\u001b[39m,dset,ntrain\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m200\u001b[39m,ntrial\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m)\n",
      "Cell \u001b[0;32mIn[20], line 66\u001b[0m, in \u001b[0;36mmain\u001b[0;34m(m_prop, losstype, dataset, ntrain, ntrial)\u001b[0m\n\u001b[1;32m     63\u001b[0m         iterator\u001b[38;5;241m.\u001b[39mset_postfix(loss\u001b[38;5;241m=\u001b[39mloss\u001b[38;5;241m.\u001b[39mitem())\n\u001b[1;32m     64\u001b[0m         optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m---> 66\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlosstype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlosstype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     67\u001b[0m model\u001b[38;5;241m.\u001b[39meval();\n\u001b[1;32m     68\u001b[0m test_preds \u001b[38;5;241m=\u001b[39m model(test_x)\u001b[38;5;241m.\u001b[39mmean\n",
      "Cell \u001b[0;32mIn[20], line 60\u001b[0m, in \u001b[0;36mmain.<locals>.train\u001b[0;34m(losstype)\u001b[0m\n\u001b[1;32m     58\u001b[0m     ym \u001b[38;5;241m=\u001b[39m train_y[order[:m]]\n\u001b[1;32m     59\u001b[0m     ystar \u001b[38;5;241m=\u001b[39m train_y[order[m:]]\n\u001b[0;32m---> 60\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[43mCondtionalMLL\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_x\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_y\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mxm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mym\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     62\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m     63\u001b[0m iterator\u001b[38;5;241m.\u001b[39mset_postfix(loss\u001b[38;5;241m=\u001b[39mloss\u001b[38;5;241m.\u001b[39mitem())\n",
      "Cell \u001b[0;32mIn[14], line 14\u001b[0m, in \u001b[0;36mCondtionalMLL\u001b[0;34m(gp, x, y, xm, ym)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mCondtionalMLL\u001b[39m(gp, x, y, xm, ym):\n\u001b[0;32m---> 14\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mget_mll\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m-\u001b[39m get_mll(gp, xm, ym)\n",
      "Cell \u001b[0;32mIn[14], line 5\u001b[0m, in \u001b[0;36mget_mll\u001b[0;34m(gp, x, y)\u001b[0m\n\u001b[1;32m      3\u001b[0m x \u001b[38;5;241m=\u001b[39m gp\u001b[38;5;241m.\u001b[39mfeature_extractor(x)\n\u001b[1;32m      4\u001b[0m x \u001b[38;5;241m=\u001b[39m gp\u001b[38;5;241m.\u001b[39mscale_to_bounds(x)\n\u001b[0;32m----> 5\u001b[0m covar_matrix \u001b[38;5;241m=\u001b[39m \u001b[43mgp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcovar_module\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m covar_matrix \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m gp\u001b[38;5;241m.\u001b[39mlikelihood\u001b[38;5;241m.\u001b[39mnoise \u001b[38;5;241m*\u001b[39m torch\u001b[38;5;241m.\u001b[39meye(N)\u001b[38;5;241m.\u001b[39mto(x\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m      7\u001b[0m log_mll \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m0.5\u001b[39m \u001b[38;5;241m*\u001b[39m (y\u001b[38;5;241m.\u001b[39mT \u001b[38;5;241m@\u001b[39m torch\u001b[38;5;241m.\u001b[39minverse(covar_matrix)) \u001b[38;5;241m@\u001b[39m y \n",
      "File \u001b[0;32m~/anaconda3/envs/opt_transport/lib/python3.12/site-packages/gpytorch/utils/deprecation.py:56\u001b[0m, in \u001b[0;36m_deprecated_renamed_method.<locals>._deprecated_method\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_deprecated_method\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     52\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m     53\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe `\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m` method is deprecated. Use `\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m` instead\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(old_method_name, new_method_name),\n\u001b[1;32m     54\u001b[0m         \u001b[38;5;167;01mDeprecationWarning\u001b[39;00m,\n\u001b[1;32m     55\u001b[0m     )\n\u001b[0;32m---> 56\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnew_method_name\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/opt_transport/lib/python3.12/site-packages/gpytorch/utils/memoize.py:59\u001b[0m, in \u001b[0;36m_cached.<locals>.g\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     57\u001b[0m kwargs_pkl \u001b[38;5;241m=\u001b[39m pickle\u001b[38;5;241m.\u001b[39mdumps(kwargs)\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _is_in_cache(\u001b[38;5;28mself\u001b[39m, cache_name, \u001b[38;5;241m*\u001b[39margs, kwargs_pkl\u001b[38;5;241m=\u001b[39mkwargs_pkl):\n\u001b[0;32m---> 59\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _add_to_cache(\u001b[38;5;28mself\u001b[39m, cache_name, \u001b[43mmethod\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;241m*\u001b[39margs, kwargs_pkl\u001b[38;5;241m=\u001b[39mkwargs_pkl)\n\u001b[1;32m     60\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _get_from_cache(\u001b[38;5;28mself\u001b[39m, cache_name, \u001b[38;5;241m*\u001b[39margs, kwargs_pkl\u001b[38;5;241m=\u001b[39mkwargs_pkl)\n",
      "File \u001b[0;32m~/anaconda3/envs/opt_transport/lib/python3.12/site-packages/gpytorch/lazy/lazy_evaluated_kernel_tensor.py:410\u001b[0m, in \u001b[0;36mLazyEvaluatedKernelTensor.to_dense\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    408\u001b[0m \u001b[38;5;129m@cached\u001b[39m\n\u001b[1;32m    409\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mto_dense\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 410\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate_kernel\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_dense\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/opt_transport/lib/python3.12/site-packages/linear_operator/utils/memoize.py:59\u001b[0m, in \u001b[0;36m_cached.<locals>.g\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     57\u001b[0m kwargs_pkl \u001b[38;5;241m=\u001b[39m pickle\u001b[38;5;241m.\u001b[39mdumps(kwargs)\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _is_in_cache(\u001b[38;5;28mself\u001b[39m, cache_name, \u001b[38;5;241m*\u001b[39margs, kwargs_pkl\u001b[38;5;241m=\u001b[39mkwargs_pkl):\n\u001b[0;32m---> 59\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _add_to_cache(\u001b[38;5;28mself\u001b[39m, cache_name, \u001b[43mmethod\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;241m*\u001b[39margs, kwargs_pkl\u001b[38;5;241m=\u001b[39mkwargs_pkl)\n\u001b[1;32m     60\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _get_from_cache(\u001b[38;5;28mself\u001b[39m, cache_name, \u001b[38;5;241m*\u001b[39margs, kwargs_pkl\u001b[38;5;241m=\u001b[39mkwargs_pkl)\n",
      "File \u001b[0;32m~/anaconda3/envs/opt_transport/lib/python3.12/site-packages/linear_operator/operators/_linear_operator.py:2601\u001b[0m, in \u001b[0;36mLinearOperator.to_dense\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2599\u001b[0m     eye \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39meye(num_cols, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdtype, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m   2600\u001b[0m     eye \u001b[38;5;241m=\u001b[39m eye\u001b[38;5;241m.\u001b[39mexpand(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_shape, num_cols, num_cols)\n\u001b[0;32m-> 2601\u001b[0m     res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmatmul\u001b[49m\u001b[43m(\u001b[49m\u001b[43meye\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2602\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m res\u001b[38;5;241m.\u001b[39mcontiguous()\n",
      "File \u001b[0;32m~/anaconda3/envs/opt_transport/lib/python3.12/site-packages/linear_operator/operators/interpolated_linear_operator.py:438\u001b[0m, in \u001b[0;36mInterpolatedLinearOperator.matmul\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m    435\u001b[0m right_interp_res \u001b[38;5;241m=\u001b[39m left_t_interp(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mright_interp_indices, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mright_interp_values, other, base_size)\n\u001b[1;32m    437\u001b[0m \u001b[38;5;66;03m# base_linear_op * right_interp^T * tensor\u001b[39;00m\n\u001b[0;32m--> 438\u001b[0m base_res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbase_linear_op\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmatmul\u001b[49m\u001b[43m(\u001b[49m\u001b[43mright_interp_res\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    440\u001b[0m \u001b[38;5;66;03m# left_interp * base_linear_op * right_interp^T * tensor\u001b[39;00m\n\u001b[1;32m    441\u001b[0m res \u001b[38;5;241m=\u001b[39m left_interp(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mleft_interp_indices, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mleft_interp_values, base_res)\n",
      "File \u001b[0;32m~/anaconda3/envs/opt_transport/lib/python3.12/site-packages/linear_operator/operators/_linear_operator.py:1831\u001b[0m, in \u001b[0;36mLinearOperator.matmul\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m   1827\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmatmul_linear_operator\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m MatmulLinearOperator\n\u001b[1;32m   1829\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m MatmulLinearOperator(\u001b[38;5;28mself\u001b[39m, other)\n\u001b[0;32m-> 1831\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mMatmul\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrepresentation_tree\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mother\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrepresentation\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/opt_transport/lib/python3.12/site-packages/torch/autograd/function.py:553\u001b[0m, in \u001b[0;36mFunction.apply\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m    550\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_are_functorch_transforms_active():\n\u001b[1;32m    551\u001b[0m     \u001b[38;5;66;03m# See NOTE: [functorch vjp and autograd interaction]\u001b[39;00m\n\u001b[1;32m    552\u001b[0m     args \u001b[38;5;241m=\u001b[39m _functorch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39munwrap_dead_wrappers(args)\n\u001b[0;32m--> 553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m    555\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_setup_ctx_defined:\n\u001b[1;32m    556\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    557\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIn order to use an autograd.Function with functorch transforms \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    558\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(vmap, grad, jvp, jacrev, ...), it must override the setup_context \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    559\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstaticmethod. For more details, please see \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    560\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://pytorch.org/docs/master/notes/extending.func.html\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    561\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/envs/opt_transport/lib/python3.12/site-packages/linear_operator/functions/_matmul.py:21\u001b[0m, in \u001b[0;36mMatmul.forward\u001b[0;34m(ctx, representation_tree, rhs, *matrix_args)\u001b[0m\n\u001b[1;32m     18\u001b[0m     is_vector \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m     20\u001b[0m linear_op \u001b[38;5;241m=\u001b[39m ctx\u001b[38;5;241m.\u001b[39mrepresentation_tree(\u001b[38;5;241m*\u001b[39mmatrix_args)\n\u001b[0;32m---> 21\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[43mlinear_op\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_matmul\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrhs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     23\u001b[0m to_save \u001b[38;5;241m=\u001b[39m [orig_rhs] \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mlist\u001b[39m(matrix_args)\n\u001b[1;32m     24\u001b[0m ctx\u001b[38;5;241m.\u001b[39msave_for_backward(\u001b[38;5;241m*\u001b[39mto_save)\n",
      "File \u001b[0;32m~/anaconda3/envs/opt_transport/lib/python3.12/site-packages/linear_operator/operators/kronecker_product_linear_operator.py:270\u001b[0m, in \u001b[0;36mKroneckerProductLinearOperator._matmul\u001b[0;34m(self, rhs)\u001b[0m\n\u001b[1;32m    267\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_vec:\n\u001b[1;32m    268\u001b[0m     rhs \u001b[38;5;241m=\u001b[39m rhs\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m--> 270\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[43m_matmul\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear_ops\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrhs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcontiguous\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_vec:\n\u001b[1;32m    273\u001b[0m     res \u001b[38;5;241m=\u001b[39m res\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/opt_transport/lib/python3.12/site-packages/linear_operator/operators/kronecker_product_linear_operator.py:42\u001b[0m, in \u001b[0;36m_matmul\u001b[0;34m(linear_ops, kp_shape, rhs)\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m linear_op \u001b[38;5;129;01min\u001b[39;00m linear_ops:\n\u001b[1;32m     41\u001b[0m     res \u001b[38;5;241m=\u001b[39m res\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m*\u001b[39moutput_batch_shape, linear_op\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m), \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m---> 42\u001b[0m     factor \u001b[38;5;241m=\u001b[39m \u001b[43mlinear_op\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_matmul\u001b[49m\u001b[43m(\u001b[49m\u001b[43mres\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     43\u001b[0m     factor \u001b[38;5;241m=\u001b[39m factor\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m*\u001b[39moutput_batch_shape, linear_op\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m), \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, num_cols)\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m     44\u001b[0m     res \u001b[38;5;241m=\u001b[39m factor\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m*\u001b[39moutput_batch_shape, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, num_cols)\n",
      "File \u001b[0;32m~/anaconda3/envs/opt_transport/lib/python3.12/site-packages/linear_operator/operators/toeplitz_linear_operator.py:44\u001b[0m, in \u001b[0;36mToeplitzLinearOperator._matmul\u001b[0;34m(self, rhs)\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_matmul\u001b[39m(\n\u001b[1;32m     41\u001b[0m     \u001b[38;5;28mself\u001b[39m: Float[LinearOperator, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*batch M N\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m     42\u001b[0m     rhs: Union[Float[torch\u001b[38;5;241m.\u001b[39mTensor, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*batch2 N C\u001b[39m\u001b[38;5;124m\"\u001b[39m], Float[torch\u001b[38;5;241m.\u001b[39mTensor, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*batch2 N\u001b[39m\u001b[38;5;124m\"\u001b[39m]],\n\u001b[1;32m     43\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Union[Float[torch\u001b[38;5;241m.\u001b[39mTensor, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m... M C\u001b[39m\u001b[38;5;124m\"\u001b[39m], Float[torch\u001b[38;5;241m.\u001b[39mTensor, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m... M\u001b[39m\u001b[38;5;124m\"\u001b[39m]]:\n\u001b[0;32m---> 44\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msym_toeplitz_matmul\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrhs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/opt_transport/lib/python3.12/site-packages/linear_operator/utils/toeplitz.py:160\u001b[0m, in \u001b[0;36msym_toeplitz_matmul\u001b[0;34m(toeplitz_column, tensor)\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msym_toeplitz_matmul\u001b[39m(toeplitz_column, tensor):\n\u001b[1;32m    152\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    153\u001b[0m \u001b[38;5;124;03m    Performs a matrix-matrix multiplication TM where the matrix T is symmetric Toeplitz.\u001b[39;00m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[38;5;124;03m        - tensor\u001b[39;00m\n\u001b[1;32m    159\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 160\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtoeplitz_matmul\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtoeplitz_column\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtoeplitz_column\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtensor\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/opt_transport/lib/python3.12/site-packages/linear_operator/utils/toeplitz.py:146\u001b[0m, in \u001b[0;36mtoeplitz_matmul\u001b[0;34m(toeplitz_column, toeplitz_row, tensor)\u001b[0m\n\u001b[1;32m    143\u001b[0m fft_c \u001b[38;5;241m=\u001b[39m fft(c_r_rev)\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m)\u001b[38;5;241m.\u001b[39mexpand_as(fft_M)\n\u001b[1;32m    144\u001b[0m fft_product \u001b[38;5;241m=\u001b[39m fft_M\u001b[38;5;241m.\u001b[39mmul_(fft_c)\n\u001b[0;32m--> 146\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mifft\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfft_product\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mreal\u001b[38;5;241m.\u001b[39mmT\n\u001b[1;32m    147\u001b[0m output \u001b[38;5;241m=\u001b[39m output[\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m, :orig_size, :]\n\u001b[1;32m    148\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for dset in {'protein','naval','concrete','winered','winewhite'}:\n",
    "    get_regression_data(dset)\n",
    "    for m in {0.1,0.25,0.5,0.75,0.9}:\n",
    "        for ntrain in {100,200,300,400,500,600,700}:\n",
    "            main(m, 'cmll',dset,ntrain,ntrial=10)\n",
    "            \n",
    "    for ntrain in {100,200,300,400,500,600,700}:\n",
    "        main(m, 'mll',dset,ntrain=200,ntrial=10)\n",
    "print('The training of the models is finished')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "opt_transport",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
